Date: 2009-08-03 14:22:39
Title: Monter un cluster filesystem avec OCFS2
Category: Talk
Tags: blog
Author: Gabès Jean
AuthorLogin: naparuba
Summary: Monter un cluster filesystem avec OCFS2

<!-- RELU -->


## Intérêt de la solution
Les architectures clusters sont de plus en plus présentes. Si nous avons vu dans de précédent post la partie réseaux avec IPVS, il reste la question des applications, et tout particulièrement du partage des données, notamment les fichiers des applications, et parfois même les datafile des bases.
C'est justement un éditeur de base de données, Oracle, qui a mis au point un cluster filesystem développé sous licence GPLv2. Les noeuds doivent avoir accès au même device qui va être "formaté" en ocfs2 pour pouvoir être monté par plusieurs noeuds à la fois. c'est un cluster file system quoi...

Pour ceux qui n'ont pas suivi, voici ce que cela donne avec deux noeuds (mais après vous pouvez en mettre bien plus) :

<img class="aligncenter size-medium wp-image-214" title="cluster filesystem" src="/images/30/cluster-filesystem.png" alt="cluster filesystem" width="300" height="255" />

Ici, le <strong>même</strong> disque est accessible des deux machines grâce à des liens SAN qui peuvent être Fibre ou iScsi (ce dernier suffit la plupart du temps). Dans cet exemple, chaque machine voit le disque sur un nom de device différent, mais ce n'est pas obligatoire.

Regardons un peu comment il fonctionner et comment on le met en place.

<!--more-->
## Mise en place
<h3>Installation</h3>
Il faut tout d'abord identifier la version de noyau qu'on utilise afin de récupérer les bons modules:

    uname -r

Il faut ensuite aller sur le site http://oss.oracle.com/projects/ocfs2/files/ et récupérer le bon RPM ocfs2.
Il faut faire de même pour le module ocfs2-tool <a title="http://oss.oracle.com/projects/ocfs2-tools/files/RedHat/RHEL4/" href="http://oss.oracle.com/projects/ocfs2-tools/files/RedHat/RHEL4/">http://oss.oracle.com/projects/ocfs2-tools/files/RedHat/RHEL4/</a>.

Uploader les deux RPM sur les serveurs et faire:

    rpm -Uvh --test ocfs2*
    rpm -Uvh ocfs2*</span>

Il faut aussi que les noeuds aient accès au même device (iScsi, SAN, autre) comme dit précédement.
### Configuration
Il faut ensuite lancer sur un des noeuds la commande graphique ocfs2console.
Pour ajouter des noeuds, faire ''Cluster-&gt;Configure nodes''. Une fois tous les noeuds ajoutés, il faut envoyer la configuration sur l'ensembles des noeuds, pour cela cliquer faire ''Cluster-&gt;Propagate Configuration''.

Attention, il faut que les serveurs puissent dialoguer entre eux par SSH avec des clés déjà positionnées pour le compte root si vous ne voulez pas retaper votre mot de passe à chaque fois.

Le fichier de configuration principal est <em>/etc/ocfs2/cluster.conf</em>.

Il faut ensuite formater le device. Le sélectionner puis faire ''Task-&gt;Format''. Attention, on peux augmenter le nombre de noeuds du cluster, mais jamais le diminuer.

En ligne de commande:
Pour formater un volume avec des blocks de 4k, 32k de cluster size sur 4 noeuds possibles:

   mkfs.ocfs2 -b 4K -C 32K -N 8 -L oracle_home /dev/sdf2

On initialise et démarre le cluster:

     /etc/init.d/o2cb configure
     /etc/init.d/o2cb start
     /etc/init.d/ocfs2 start

Pour monter le filesystem:

    mount -t ocfs2 /dev/sdf2 /u01
Pour le démonter:

    umount /u01

Pour certaines applications il faut des montages particuliers. Par exemple, les Datafiles ou tout ce qui y ressemble doivent avoir les options suivantes:
 *<em>datavolume</em> : force les process à avoir le flag o_direct
 *<em>nointr</em> : permet d'enpécher les interuptions par signaux des lectures et écritures

Exemple de montage:

    mount -t ocfs2 -o datavolume,nointr /dev/sdf2 /u01
    mount
    /dev/sdf2 on /u01 type ocfs2 (rw,datavolume,nointr)
    cat /etc/fstab
    /dev/sdf2 /u01 ocfs2 _netdev,datavolume,nointr 0 0
    /dev/sdg2 /orahome ocfs2 _netdev 0 0</span>

L'option <em> _netdev</em> permet de demander le montage après que le réseau soit up et de même pour l'arrêt.

Afin d'avoir le lancement d'ocfs2 au démarrage:

    chkconfig --add ocfs2
    chkconfig --add o2cb
    /etc/init.d/o2cb configure
    Load O2CB driver on boot (y/n) [n]: y
    Cluster to start on boot (Enter "none" to clear) []: ocfs2
    Writing O2CB configuration: OK</span>
    To mount by label, do:
    mount -L datafiles /u01

## Administration
Pour voir l'état du cluster:

    /etc/init.d/o2cb status
    Module "configfs": Not loaded
    Filesystem "configfs": Not mounted
    Module "ocfs2_nodemanager": Not loaded
    Module "ocfs2_dlm": Not loaded
    Module "ocfs2_dlmfs": Not loaded
    Filesystem "ocfs2_dlmfs": Not mounted</span>

Pour mettre le cluster online:

    /etc/init.d/o2cb online ocfs2
    Starting cluster ocfs2: OK

Pour mettre le cluster offline:

    /etc/init.d/o2cb offline ocfs2
    Cleaning heartbeat on ocfs2: OK
    Stopping cluster ocfs2: OK

Pour ajouter o2cb au lancement:

     /etc/init.d/o2cb configure
    Configuring the O2CB driver.
    This will configure the on-boot properties of the O2CB driver.
    The following questions will determine whether the driver is loaded on
    boot. The current values will be shown in brackets ('[]'). Hitting
    &lt;ENTER&gt; without typing an answer will keep that current value. Ctrl-C
    will abort.
    Load O2CB driver on boot (y/n) [n]: y
    Cluster to start on boot (Enter "none" to clear) []: ocfs2
    Writing O2CB configuration: OK</span>

Une fois configuré, le lancement/arrêt du cluster se fait par:

    /etc/init.d/o2cb start
    Loading module "configfs": OK
    Mounting configfs filesystem at /config: OK
    Loading module "ocfs2_nodemanager": OK
    Loading module "ocfs2_dlm": OK
    Loading module "ocfs2_dlmfs": OK
    Mounting ocfs2_dlmfs filesystem at /dlm: OK
    Starting cluster ocfs2: OK
    /etc/init.d/o2cb stop
    Cleaning heartbeat on ocfs2: OK
    Stopping cluster ocfs2: OK
    Unmounting ocfs2_dlmfs filesystem: OK
    Unloading module "ocfs2_dlmfs": OK
    Unmounting configfs filesystem: OK
    Unloading module "configfs": OK

### Changer le nombre de noeuds d'un filesystem
<p style="text-align: left;">Pour augmenter le nombre de montages possible d'un même filesystem OCFS, il faut utiliser la commande tunefs.ocfs2. Il faut le faire cluster arrété.

    umount /montage
    /etc/init.d/ocfs2 stop
    tunefs.ocfs2 -N 16 /dev/emcpowerd
    (Répondre y)
    /etc/init.d/ocfs2 start
    mount -a</span>
### Modifications sur les configurations des noeuds
La seule opération qui peut se faire à chaud est le rajout de noeuds. En cas de changement dans les IP des noeuds ou bien le changement des ports de communications, il faut que le cluster soit offline.
### Rajout d'un noeud au cluster
Cette opération peut se faire à chaud. Il faut Lancer la commande suivante sur <strong>TOUS LES NOEUDS</strong>.

    o2cb_ctl -C -i -n NODENAME -t node -a number=NODENUM -a ip_address=IPADDR -a ip_port=IPPORT -a cluster=CLUSTERNAME</span>

Copier ensuite un fichier /<em>etc/ocfs2/cluster.conf</em> d'un des noeuds sur le nouveau.
### Le cluster ne veux pas passer offline
Dans ce cas, il faut vérifier que les informations des fichiers <em>cluster.conf</em> sont bonnes.
### Redimensionnement d'un file system OCFS2
On peut agrandir un filesystem OCFS2 avec la commande tunefs.ocfs2. Attention, ceci permet de resizer le filesystem, pas le volume.

    tunefs.ocfs2 -S /dev/sdX
Afin de redimensionner à la taille max mais attention : le redimensionnement est offline.
### Voir l'agrandissement des filesystem sur les autres noeuds
Sans avoir besoin de redémarrer:
    blockdev --rereadpt /dev/sdX
### <span style="font-weight: normal; font-size: 13px;">Limitations</span>

 * Le nombre de sous répertoires dans un répertoire est limité à 32000.
 * La taille d'un file system est limité à (2 ^ 32) * blocksize. Avec des blocks à 4KB ceci donne 16TB.

### Connectivité réseau
Un noeud va considérer un autre vivant lorsqu'il réponds sur le port TCP sans avoir un timeout d'idle de O2CB_IDLE_TIMEOUT_MS
### Super blocks
Un super block est un block décrivant le filesystem. Il est indispensable, et le perdre est plus qu'embêtant. On peut donc (doit!) le backuper. Ocfs2 en fait des backup aux adresses  1G, 4G, 16G, 64G, 256G et 1T byte

Pour voir ces backups:

    debugfs.ocfs2 -R "stats" /dev/sdX | grep "Feature Compat"
    Feature Compat: 1 BackupSuper</span>

Récupération du backup:

    fsck.ocfs2 -f -r 2 /dev/sdX</span>
### Configuration des timeouts
Les différents timeouts sont:

 * O2CB_HEARTBEAT_THRESHOLD : timeout de didk. Nombre de ticks (2secondes) avant qu'un noeud soit déclaré mort. O2CB_HEARTBEAT_THRESHOLD = (((timeout in seconds) / 2) + 1). Par exemple, pour donner un timeout a 60 secondes, il faut le placer a 31. C'est la valeur par défaut.
 * O2CB_IDLE_TIMEOUT_MS : timeout d'idle réseau. Nombre de millisecondes d'idle pour qu'un noeud soit déclaré unreachable. La valeur par défaut est de 30s (30000).
 * O2CB_KEEPALIVE_DELAY_MS : timeout de keep alive réseau. C'est le délai maximum en millisecondes avant qu'un paquet keepalive soit envoyé. La valeur par défaut est de 2000ms (2s).
 * O2CB_RECONNECT_DELAY_MS : C'est le delai minimum en millisecondes entre les tentavies de connexions. Par défaut 2000ms (2s).

On peut voir ces valeurs dans <em>/etc/sysconfig/o2cb</em>.

    cat /etc/sysconfig/o2cb</span>
